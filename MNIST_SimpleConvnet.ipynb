{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW6qIjnIeXe07mHw34vF2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iny045/Quantization-Aware-Training/blob/main/MNIST_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aaz0Pe0PVxwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "def compute_ternary_params_groupwise(w: torch.Tensor,\n",
        "                                     group_size: int = 128,\n",
        "                                     m: float = 0.75):\n",
        "    assert w.shape[1] % group_size == 0, \\\n",
        "        \"in_features must be divisible by group_size\"\n",
        "\n",
        "    w_abs = w.abs().reshape(-1, group_size)          # (groups, group_size)\n",
        "\n",
        "    # threshold Δ per group\n",
        "    delta = m * w_abs.mean(dim=1)       # (groups,)\n",
        "\n",
        "    # mask of weights that survive the threshold\n",
        "    keep   = w_abs > delta.unsqueeze(1)              # broadcast\n",
        "\n",
        "    # scale α  = mean(|w| over kept  fallback to Δ if all-zero\n",
        "    keep_cnt = keep.sum(dim=1)                       # (groups,)\n",
        "    keep_sum = (w_abs * keep).sum(dim=1)\n",
        "    scale = torch.where(keep_cnt > 0,\n",
        "                        keep_sum / keep_cnt,\n",
        "                        delta.detach())              # avoid divide-by-0\n",
        "\n",
        "    return delta, scale\n",
        "\n",
        "\n",
        "def quantize_ternary_groupwise(w: torch.Tensor,\n",
        "                               delta: torch.Tensor,\n",
        "                               group_size: int = 128):\n",
        "    w_r    = w.reshape(-1, group_size)               # (groups, group_size)\n",
        "    delta  = delta.unsqueeze(1)                      # (groups, 1)\n",
        "    qw_r   = torch.where(\n",
        "        w_r >  delta,  1.0,\n",
        "        torch.where(w_r < -delta, -1.0, 0.0)\n",
        "    )\n",
        "    return qw_r.reshape_as(w)\n",
        "\n",
        "def dequantize_ternary_groupwise(qw: torch.Tensor,\n",
        "                                 scale: torch.Tensor,\n",
        "                                 group_size: int = 128):\n",
        "    qw_r  = qw.reshape(-1, group_size)\n",
        "    scale = scale.unsqueeze(1)                       # (groups, 1)\n",
        "    return (qw_r * scale).reshape_as(qw)\n",
        "\n",
        "class GroupTernaryFakeQuant(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, w: torch.Tensor,\n",
        "                group_size: int = 128,\n",
        "                m: float = 0.75):\n",
        "        delta, scale = compute_ternary_params_groupwise(\n",
        "            w, group_size, m=m\n",
        "        )\n",
        "        qw  = quantize_ternary_groupwise(w,   delta,  group_size)\n",
        "        dqw = dequantize_ternary_groupwise(qw, scale, group_size)\n",
        "\n",
        "        ctx.save_for_backward(scale)   # only scale needed for STE\n",
        "        return dqw\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        #ste\n",
        "        return grad_output, None, None\n",
        "\n",
        "def compute_int_scale(x: torch.Tensor, bitwidth: int):\n",
        "    qmax = 2**bitwidth - 1\n",
        "    scale = x.abs().max() / qmax\n",
        "    return scale\n",
        "\n",
        "def quantize_int(x: torch.Tensor, scale: torch.Tensor, bitwidth: int):\n",
        "    qmax = 2**bitwidth - 1\n",
        "    qx = torch.round(x / scale)\n",
        "    qx = torch.clamp(qx, -qmax, qmax)\n",
        "    return qx\n",
        "\n",
        "def dequantize_int(qx: torch.Tensor, scale: torch.Tensor):\n",
        "    dqx = qx * scale\n",
        "    return dqx\n",
        "\n",
        "\n",
        "class PerTensorInt8QFakeQuant(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        scale = compute_int_scale(x, bitwidth=8)\n",
        "        qx = quantize_int(x, scale, bitwidth=8)\n",
        "        dqx = dequantize_int(qx, scale)\n",
        "        ctx.save_for_backward(scale)\n",
        "        return dqx\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        scale = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "def fakequantize_weight_ternary(w,\n",
        "                                group_size: int = 128,\n",
        "                                m: float = 0.75):\n",
        "    return GroupTernaryFakeQuant.apply(w, group_size, m)\n",
        "\n",
        "def fakequantize_activation_int8(w):\n",
        "    return PerTensorInt8QFakeQuant.apply(w)\n",
        "\n",
        "\n",
        "class TernaryConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True, group_size=128, m=0.75):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "        self.group_size = group_size\n",
        "        self.m = m\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size,\n",
        "            stride=stride, padding=padding, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        quant_weight = fakequantize_weight_ternary(\n",
        "            self.conv.weight,\n",
        "            group_size=self.group_size,\n",
        "            m=self.m\n",
        "        )\n",
        "\n",
        "        return F.conv2d(\n",
        "            x, quant_weight, self.conv.bias,\n",
        "            self.stride, self.padding,\n",
        "            self.dilation, self.groups\n",
        "        )\n",
        "\n",
        "class TernaryLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, group_size=128, m=0.75):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.group_size = group_size\n",
        "        self.m = m\n",
        "\n",
        "        self.linear = nn.Linear(in_features, out_features, bias = bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        quant_weight = fakequantize_weight_ternary(\n",
        "            self.linear.weight,\n",
        "            group_size=self.group_size,\n",
        "            m=self.m\n",
        "        )\n",
        "\n",
        "        return F.linear(x, quant_weight, self.linear.bias)\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, group_size=128, m=0.75):\n",
        "        super().__init__()\n",
        "        self.conv1 = TernaryConv2d(1, 32, (3, 3), 1, group_size=group_size, m=m)\n",
        "        self.conv2 = TernaryConv2d(32, 64, (3, 3), 1, group_size=group_size, m=m)\n",
        "\n",
        "        self.fc1 = TernaryLinear(64*5*5, 128, group_size=group_size, m=m)\n",
        "        self.fc2 = TernaryLinear(128, num_classes, group_size=group_size, m=m)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = fakequantize_activation_int8(x)\n",
        "\n",
        "        x = F.relu(self.conv1(x)) # this calls the forward method of class TernaryConv2d\n",
        "        x = self.pool(x)\n",
        "        x = fakequantize_activation_int8(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = fakequantize_activation_int8(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = fakequantize_activation_int8(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data) #forward function of classSimpleCOnvnet is called\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({accuracy:.0f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    lr = 0.01\n",
        "    momentum = 0.9\n",
        "    group_size = 1 # Changed group_size to 1\n",
        "    m = 0.75\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)# loads the data in batches for memory efficiency\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    model = SimpleConvNet(group_size=group_size, m=m).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch)\n",
        "        accuracy = test(model, device, test_loader)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), \"ternary_convnet_best.pth\")\n",
        "\n",
        "    print(f\"Best accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "6O1JD_fIY-X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d0dd35-091c-4550-da73-c4170e2d261a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307500\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.396285\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.144319\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.255003\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.280986\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.292895\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.159164\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.119963\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.148097\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.045865\n",
            "\n",
            "Test set: Average loss: 0.0547, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.347524\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.081635\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.037388\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.066275\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.193423\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.212190\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.044072\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.013329\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.037572\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.106459\n",
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 9866/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.041608\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.081765\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.010322\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.088878\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.047855\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.206175\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.084270\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.026200\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.108867\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.021009\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9898/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.013613\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.030044\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.039960\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.023278\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.018170\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.041028\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.077940\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.026958\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.020130\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.037786\n",
            "\n",
            "Test set: Average loss: 0.0267, Accuracy: 9918/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.013673\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.075498\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.060365\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.012029\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.052512\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.035493\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.197546\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.161031\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.048589\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003657\n",
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 9916/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.053644\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.018056\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.012288\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.004458\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.042034\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006709\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.002623\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.019394\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.135815\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.005446\n",
            "\n",
            "Test set: Average loss: 0.0241, Accuracy: 9928/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.031973\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.073527\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.016427\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010951\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.021433\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.018052\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.016152\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.050831\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.028382\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.011429\n",
            "\n",
            "Test set: Average loss: 0.0258, Accuracy: 9923/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.009508\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.106084\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.017336\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.013981\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001438\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.022165\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.024267\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.037540\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.116312\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.008130\n",
            "\n",
            "Test set: Average loss: 0.0245, Accuracy: 9916/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.130688\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.003407\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.010205\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.015263\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.006754\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.005028\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.003061\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.005650\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.013020\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.003370\n",
            "\n",
            "Test set: Average loss: 0.0253, Accuracy: 9916/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.003899\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001875\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.012976\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.036086\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.053437\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002256\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002285\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.006915\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.006308\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.019657\n",
            "\n",
            "Test set: Average loss: 0.0232, Accuracy: 9926/10000 (99%)\n",
            "\n",
            "Best accuracy: 99.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBDJm3qkDH0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
